---
title: 'Bulk Operations'
description: 'Efficiently manage large product catalogs'
---

## Bulk Upsert

Create or update up to 1000 items per request:

```python
response = requests.post(
    f"{BASE_URL}/public/catalogs/{catalog_id}/items/bulk",
    headers=headers,
    json={
        "items": products,  # Up to 1000 items
        "unique_field": "sku"
    }
)
```

## Batching Large Datasets

For >1000 items, split into batches:

```python
def bulk_sync(products, batch_size=500):
    """Sync products in batches."""
    results = {"created": 0, "updated": 0, "failed": 0}

    for i in range(0, len(products), batch_size):
        batch = products[i:i+batch_size]

        response = requests.post(
            f"{BASE_URL}/public/catalogs/{catalog_id}/items/bulk",
            headers=headers,
            json={"items": batch, "unique_field": "sku"}
        )

        r = response.json()
        results["created"] += r["created"]
        results["updated"] += r["updated"]
        results["failed"] += r.get("failed", 0)

    return results
```

## Parallel Processing

Speed up large syncs with concurrent requests:

```python
import concurrent.futures

def sync_batch(batch):
    return requests.post(
        f"{BASE_URL}/public/catalogs/{catalog_id}/items/bulk",
        headers=headers,
        json={"items": batch, "unique_field": "sku"}
    ).json()

# Split into batches
batches = [products[i:i+500] for i in range(0, len(products), 500)]

# Process in parallel (max 5 concurrent)
with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
    results = list(executor.map(sync_batch, batches))

# Aggregate results
total_created = sum(r["created"] for r in results)
total_updated = sum(r["updated"] for r in results)
print(f"Created: {total_created}, Updated: {total_updated}")
```

## Error Handling

Handle partial failures gracefully:

```python
response = requests.post(
    f"{BASE_URL}/public/catalogs/{catalog_id}/items/bulk",
    headers=headers,
    json={"items": products, "unique_field": "sku"}
)

result = response.json()

if result.get("failed", 0) > 0:
    print(f"Warning: {result['failed']} items failed")
    # Optionally retry failed items

print(f"Success: {result['created']} created, {result['updated']} updated")
```

## Best Practices

1. **Batch size**: Use 500-1000 items per request
2. **Retry logic**: Implement exponential backoff for failures
3. **Idempotency**: Bulk upsert is idempotent - safe to retry
4. **Progress tracking**: Log progress for long-running syncs
